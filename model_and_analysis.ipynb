{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SLEEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors.graph import kneighbors_graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from collections import namedtuple\n",
    "\n",
    "from ensemble import Model, Ensemble\n",
    "from helpers import precision_at_ks, print_hdf5_object, project\n",
    "from core import learn_V\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "import implicit.als\n",
    "X=np.random.randint(5, size=(100,200))\n",
    "X0=coo_matrix(X, dtype=np.float64)\n",
    "model = implicit.als.AlternatingLeastSquares(factors=50)\n",
    "model.fit(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew install gcc\n",
    "# !pip install implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('data/train_X.csv')\n",
    "train_Y = pd.read_csv('data/train_Y.csv')\n",
    "val_X = pd.read_csv('data/val_X.csv')\n",
    "val_Y = pd.read_csv('data/val_Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.drop(columns = ['ex_id'])\n",
    "train_Y = train_Y.drop(columns = ['ex_id'])\n",
    "val_X = val_X.drop(columns = ['ex_id'])\n",
    "val_Y = val_Y.drop(columns = ['ex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y = csc_matrix(train_X), csc_matrix(train_Y)\n",
    "val_X, val_Y = csc_matrix(val_X), csc_matrix(val_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<15539x5000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 3684745 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = namedtuple('args', ['num_learner', 'num_clusters',\n",
    "                             'num_threads', 'SVP_neigh', 'out_dim',\n",
    "                             'w_thresh', 'sp_thresh', 'cost',\n",
    "                             'NNtest', 'normalize'])\n",
    "params.num_learners = 1 \n",
    "params.num_clusters = 1\n",
    "params.num_threads = 32\n",
    "params.SVP_neigh = 250\n",
    "params.out_Dim = 100\n",
    "params.w_thresh = 0.01\n",
    "params.sp_thresh = 0.01\n",
    "params.NNtest = 25\n",
    "params.normalize = 1\n",
    "params.regressor_lambda1 = 1e-6\n",
    "params.regressor_lambda2 = 1e-3\n",
    "params.embedding_lambda = 0.1  # determined automatically in WAltMin_asymm.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterings = []\n",
    "for i in range(params.num_learners):\n",
    "    model = KMeans(n_clusters=params.num_clusters, n_jobs=-1, n_init=8, max_iter=100)\n",
    "    model.fit(train_X)\n",
    "    clusterings.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_model = clusterings[0]\n",
    "models = []\n",
    "i = 0\n",
    "data_idx = np.nonzero(clus_model.labels_ == i)[0]\n",
    "X = train_X[data_idx, :]\n",
    "Y = train_Y[data_idx, :]\n",
    "graph = kneighbors_graph(Y, params.SVP_neigh, mode='distance', metric='cosine',\n",
    "                         include_self=True,\n",
    "                         n_jobs=-1)\n",
    "graph.data = 1 - graph.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MKL_NUM_THREADS'] = '1'\n",
    "os.environ['export OPENBLAS_NUM_THREADS'] = '1'\n",
    "\n",
    "als_model = implicit.als.AlternatingLeastSquares(factors=params.out_Dim,\n",
    "                                                 regularization=params.embedding_lambda)\n",
    "als_model.fit(graph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = als_model.item_factors\n",
    "regressor = ElasticNet(alpha=0.1, l1_ratio=0.001)\n",
    "regressor.fit(X, Z)\n",
    "V = regressor.coef_\n",
    "fitted_Z = X.toarray() @ V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_neighbors = NearestNeighbors(n_neighbors=params.NNtest, metric='cosine').fit(fitted_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_center = project(V, clus_model.cluster_centers_[i])\n",
    "learned = {'center_z': projected_center,\n",
    "           'V': V,\n",
    "           'Z_neighbors': Z_neighbors,\n",
    "           'data_idx': data_idx}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "ensemble = pickle.load(open('sleec_default_2.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "from helpers import project\n",
    "\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, cluster_models, train_Y):\n",
    "        self.models = cluster_models\n",
    "        self.train_Y = train_Y\n",
    "        \n",
    "    def closet_cluster(self, x):\n",
    "        sims = []\n",
    "        for m in self.models:\n",
    "            z = project(m['V'], x)  # the projected value\n",
    "            sim = cosine_similarity([z], [m['center_z']])\n",
    "            sims.append(sim)\n",
    "        return self.models[np.argmax(sims)]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        model = self.closet_cluster(x)\n",
    "        z = project(model['V'], x)\n",
    "        dist, nbrs = model['Z_neighbors'].kneighbors([z], return_distance=True)\n",
    "        real_idx = [model['data_idx'][i] for i in nbrs[0]]\n",
    "\n",
    "        # weight by 1 / distance\n",
    "        dist += 1e-10\n",
    "        weights = (1 / dist).T\n",
    "        labels = np.asarray(self.train_Y[real_idx, :].todense())\n",
    "        # print(weights.shape)\n",
    "        # print(labels.shape)\n",
    "        # print(type(weights))\n",
    "        # print(type(labels))\n",
    "        scores_per_instance = labels * weights\n",
    "        scores = scores_per_instance.sum(axis=0)\n",
    "        return np.array(scores).flatten()\n",
    "\n",
    "learners = ensemble.models[0].models\n",
    "models = [Model([learner], train_Y) for learner in learners]\n",
    "ensemble_new = Ensemble(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1316/1316 [00:20<00:00, 65.20it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_Y_new = ensemble_new.predict_many(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_Y = pd.read_csv('data/val_Y.csv')\n",
    "val_Y = val_Y.drop(columns = ['ex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49119083483312037"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import label_ranking_average_precision_score\n",
    "label_ranking_average_precision_score(val_Y.to_numpy(), pred_Y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-vs-all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '/Users/yuema/Desktop/DSGA1003/Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.neighbors.graph import kneighbors_graph\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from collections import namedtuple\n",
    "\n",
    "from sleec.ensemble import Model, Ensemble\n",
    "from sleec.helpers import precision_at_ks, print_hdf5_object, project\n",
    "from sleec.core import learn_V\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('train_X.csv')\n",
    "train_Y = pd.read_csv('train_Y.csv')\n",
    "val_X = pd.read_csv('val_X.csv')\n",
    "val_Y = pd.read_csv('val_Y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.drop(columns = ['ex_id'])\n",
    "train_Y = train_Y.drop(columns = ['ex_id'])\n",
    "val_X = val_X.drop(columns = ['ex_id'])\n",
    "val_Y = val_Y.drop(columns = ['ex_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "clf = OneVsRestClassifier(XGBClassifier(random_state=123)).fit(train_X, train_Y)\n",
    "predictions = clf.predict(val_X)\n",
    "end = time.time() - start\n",
    "np.savetxt('/content/gdrive/My Drive/1003 Project/OvA_boosting_default.csv', predictions)\n",
    "model_file_path='/content/gdrive/My Drive/1003 Project/OvA_boosting_default.pickle'\n",
    "pickle.dump(clf, open(model_file_path, 'wb'))\n",
    "\n",
    "print('Time need for model fitting: {} hrs'.format(end/3600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
